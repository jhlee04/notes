```
from sklearn.metrics import classification_report
import matplotlib.pyplot as plt

# classification_report → 딕셔너리 변환
from sklearn.metrics import precision_recall_fscore_support

labels = ['on-track', 'at-risk']
zero_f1 = precision_recall_fscore_support(eval_df['정답'], eval_df['zero-shot'], labels=labels, zero_division=0)[2]
s3_f1 = precision_recall_fscore_support(eval_df['정답'], eval_df['status3'], labels=labels, zero_division=0)[2]

x = range(len(labels))  # 0: on-track, 1: at-risk
width = 0.35

plt.figure(figsize=(8, 5))
bars1 = plt.bar([i - width/2 for i in x], zero_f1, width=width, label='Zero-shot')
bars2 = plt.bar([i + width/2 for i in x], s3_f1, width=width, label='LLM status3')

# 수치 표시
for bar in bars1 + bars2:
    height = bar.get_height()
    plt.text(bar.get_x() + bar.get_width()/2, height + 0.01, f"{height:.2f}", ha='center', va='bottom')

plt.xticks(x, labels)
plt.ylim(0, 1.05)
plt.ylabel("F1-score")
plt.title("상태별 F1-score 비교 (Zero-shot vs Status3)")
plt.legend()
plt.grid(axis='y', linestyle='--', alpha=0.5)
plt.tight_layout()
plt.show()

```



아주 좋은 질문입니다.
정확도뿐 아니라 F1-score 해석 기준, 그래프 수치 표현, 클래스별 정확도 분석까지 요청하신 내용을 모두 반영해 아래에 정리해드릴게요.


---

✅ 1. F1-score는 얼마나 높아야 “좋은” 성능인가?

F1 Score 범위	해석

0.90 ~ 1.00	매우 우수한 모델 (거의 완벽)
0.80 ~ 0.89	실무 적용에 충분히 우수
0.70 ~ 0.79	개선 여지가 있는 실용 모델
0.60 ~ 0.69	기준선 정도 (Baseline 수준)
< 0.60	불안정한 모델, 재구성 필요


→ 0.8 이상이면 실무 기준에서는 “신뢰할 수 있다”는 인식으로 봐도 좋습니다.
특히 status 판단처럼 모호할 수 있는 분류 문제에서는 0.75~0.85도 매우 의미 있습니다.


---

✅ 2. on-track과 at-risk 각각의 분류 정확도 (정밀도/재현율)도 표현하기

→ classification_report() 또는 클래스별 precision, recall, f1-score 출력으로 가능


---

✅ 3. 정확도 & F1-score 시각화 (막대그래프 + 수치 표기)
```
import matplotlib.pyplot as plt

# 정확도 및 F1 점수
labels = ['Zero-shot', 'LLM status3']
accs = [acc_zero, acc_s3]
f1s = [f1_zero, f1_s3]

x = range(len(labels))
width = 0.35

plt.figure(figsize=(8, 5))
bars1 = plt.bar(x, accs, width=width, label='Accuracy')
bars2 = plt.bar([i + width for i in x], f1s, width=width, label='F1-score')

# 수치 표시
for bar in bars1 + bars2:
    height = bar.get_height()
    plt.text(bar.get_x() + bar.get_width() / 2, height + 0.01, f"{height:.2f}", ha='center', va='bottom')

plt.xticks([i + width / 2 for i in x], labels)
plt.ylim(0, 1.05)
plt.ylabel("Score")
plt.title("LLM 상태 예측 정확도 / F1-score 비교")
plt.legend()
plt.grid(axis='y', linestyle='--', alpha=0.5)
plt.tight_layout()
plt.show()

```
---

✅ 4. on-track vs at-risk 각각의 분류 성능 보기
```
from sklearn.metrics import classification_report

print("🔎 Zero-shot 모델 성능:")
print(classification_report(eval_df['정답'], eval_df['zero-shot'], target_names=['on-track', 'at-risk']))

print("\n🔎 Status3 모델 성능:")
print(classification_report(eval_df['정답'], eval_df['status3'], target_names=['on-track', 'at-risk']))
```
이렇게 하면 다음과 같은 텍스트 출력이 나옵니다:

precision    recall  f1-score   support

   on-track       0.87      0.91      0.89        45
    at-risk       0.83      0.76      0.79        34

    accuracy                           0.85        79


---

✅ 결론 정리

항목	권장 여부	이유

Accuracy만 보는 것	❌ 부족	불균형일 경우 왜곡 가능성
F1-score 포함	✅ 필수	전체 균형 판단 가능
on-track / at-risk 별 성능 분리	✅ 매우 권장	어떤 상태에서 오분류가 많은지 진단 가능
수치 시각화 표시	✅ 추천	리더·사용자에게 직관적 전달 가능



---
 
